{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the necessary methods from tweepy library  \n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime, date, time, timedelta\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "#install tweepy if you don't have it\n",
    "#!pip install tweepy\n",
    "import tweepy\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "\n",
    "#sentiment analysis package\n",
    "#!pip install textblob\n",
    "from textblob import TextBlob\n",
    "\n",
    "#general text pre-processor\n",
    "#!pip install nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "\n",
    "#tweet pre-processor \n",
    "#!pip install tweet-preprocessor\n",
    "import preprocessor as p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is a basic listener that writes received tweets to file.\n",
    "class StdOutListener(StreamListener):\n",
    "\n",
    "    def __init__(self,fhandle, stop_at = 1000):\n",
    "        self.tweet_counter = 0\n",
    "        self.stop_at = stop_at\n",
    "        self.fhandle = fhandle\n",
    "         \n",
    "        \n",
    "    def on_data(self, data):\n",
    "        self.fhandle.write(data)\n",
    "        \n",
    "        #stop if enough tweets are obtained\n",
    "        self.tweet_counter += 1   \n",
    "        if self.tweet_counter < self.stop_at:        \n",
    "            return True\n",
    "        else:\n",
    "            print('Max number of tweets reached: #tweets = ' + str(self.tweet_counter))\n",
    "            return False\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print (status)\n",
    "\n",
    "def stream_tweet_data(filename='data/tweets.json',\n",
    "                      keywords=['COVID19KE'],\n",
    "                      is_async=False):\n",
    "    # tweet topics to use as a filter. The tweets downloaded\n",
    "    # will have one of the topics in their text or hashtag \n",
    "\n",
    "    print('saving data to file: ',filename)\n",
    "\n",
    "    #print the tweet topics \n",
    "    print('Tweet Keywords are: ',keywords)\n",
    "    print('For testing case, please interupt the downloading process \\\n",
    "            using ctrl+x after about 5 mins ')\n",
    "    print('To keep streaming in the background, pass is_async=True')\n",
    "\n",
    "    #Variables that contains the user credentials to access Twitter API \n",
    "    consumer_key = os.environ.get('TWITTER_API_KEY')\n",
    "    consumer_secret = os.environ.get('TWITTER_API_SECRET')\n",
    "    access_token = os.environ.get('TWITTER_ACCESS_TOKEN')\n",
    "    access_token_secret = os.environ.get('TWITTER_ACCESS_TOKEN_SECRET')\n",
    "    \n",
    "\n",
    "    #open file \n",
    "    fhandle=open(filename,'w')\n",
    "\n",
    "    #This handles Twitter authetification and the connection to Twitter Streaming API\n",
    "    l = StdOutListener(fhandle)\n",
    "    auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "    stream = Stream(auth, l)\n",
    "\n",
    "    #This line filter Twitter Streams to capture data by the keywords: first argument to this code\n",
    "    stream.filter(track=keywords,is_async=is_async)\n",
    "\n",
    "    return None\n",
    "\n",
    "def read_tweet_json(tweets_file,verbose=1):\n",
    "    '''\n",
    "    Read a twitter raw json dumped file into a list.\n",
    "    It attempts two methods:\n",
    "        1) read line by line and parse each line using json.loads\n",
    "        2) assuming there was no \\n used, load everything, replace \n",
    "    '''\n",
    "    #\n",
    "    ii = 0\n",
    "    try:     \n",
    "        print('trying method 1')\n",
    "        tweets_data = []\n",
    "        for line in open(tweets_file, \"r\"):        \n",
    "            tweet = json.loads(line.strip())                \n",
    "            tweets_data.append(tweet)\n",
    "             \n",
    "    except:\n",
    "        print('trying method 2')\n",
    "        with open(tweets_file) as f:\n",
    "            tweets_data = json.loads(\"[\" + f.read().replace(\"}\\n{\", \"},\\n{\") + '{}'+ \"]\")        \n",
    "            \n",
    "    #\n",
    "    if verbose>0:\n",
    "        print(f'#tweets from {tweets_file}: {len(tweets_data)}')\n",
    "        \n",
    "    return tweets_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class tweetsearch():\n",
    "    '''\n",
    "    This is a basic class to search and download twitter data.\n",
    "    You can build up on it to extend the functionalities for more \n",
    "    sophisticated analysis\n",
    "    '''\n",
    "    def __init__(self, cols=None,auth=None, **kwargs):\n",
    "        #\n",
    "        \n",
    "        self.verbose = kwargs.get('verbose',1)\n",
    "        \n",
    "        #\n",
    "        if not cols is None:\n",
    "            self.cols = cols\n",
    "        else:\n",
    "            self.cols = ['id', 'created_at', 'source', 'original_text','clean_text', \n",
    "                    'sentiment','polarity','subjectivity', 'lang',\n",
    "                    'favorite_count', 'retweet_count', 'original_author',   \n",
    "                    'possibly_sensitive', 'hashtags',\n",
    "                    'user_mentions', 'place', 'place_coord_boundaries']\n",
    "            \n",
    "        if auth is None:\n",
    "            \n",
    "            #Variables that contains the user credentials to access Twitter API \n",
    "            consumer_key = os.environ.get('TWITTER_API_KEY')\n",
    "            consumer_secret = os.environ.get('TWITTER_API_SECRET')\n",
    "            access_token = os.environ.get('TWITTER_ACCESS_TOKEN')\n",
    "            access_token_secret = os.environ.get('TWITTER_ACCESS_TOKEN_SECRET')\n",
    "            \n",
    "\n",
    "\n",
    "            #This handles Twitter authetification and the connection to Twitter Streaming API\n",
    "            auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "            auth.set_access_token(access_token, access_token_secret)\n",
    "            \n",
    "\n",
    "        #            \n",
    "        self.auth = auth\n",
    "        self.api = tweepy.API(auth,wait_on_rate_limit=True) \n",
    "        self.filtered_tweet = ''\n",
    "            \n",
    "    def limit_status(self):\n",
    "        return self.api.rate_limit_status()\n",
    "\n",
    "    def clean_tweets(self, twitter_text):\n",
    "\n",
    "        #use pre processor\n",
    "        tweet = p.clean(twitter_text)\n",
    "\n",
    "         #HappyEmoticons\n",
    "        emoticons_happy = set([\n",
    "            ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "            ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "            '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "            'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "            '<3'\n",
    "            ])\n",
    "\n",
    "        # Sad Emoticons\n",
    "        emoticons_sad = set([\n",
    "            ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "            ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "            ':c', ':{', '>:\\\\', ';('\n",
    "            ])\n",
    "\n",
    "        #Emoji patterns\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "                 u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                 u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                 u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                 u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                 u\"\\U00002702-\\U000027B0\"\n",
    "                 u\"\\U000024C2-\\U0001F251\"\n",
    "                 \"]+\", flags=re.UNICODE)\n",
    "\n",
    "        #combine sad and happy emoticons\n",
    "        emoticons = emoticons_happy.union(emoticons_sad)\n",
    "\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        word_tokens = nltk.word_tokenize(tweet)\n",
    "        #after tweepy preprocessing the colon symbol left remain after      \n",
    "        #removing mentions\n",
    "        tweet = re.sub(r':', '', tweet)\n",
    "        tweet = re.sub(r'‚Ä¶', '', tweet)\n",
    "\n",
    "        #replace consecutive non-ASCII characters with a space\n",
    "        tweet = re.sub(r'[^\\x00-\\x7F]+',' ', tweet)\n",
    "\n",
    "        #remove emojis from tweet\n",
    "        tweet = emoji_pattern.sub(r'', tweet)\n",
    "\n",
    "        #filter using NLTK library append it to a string\n",
    "        filtered_tweet = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "        #looping through conditions\n",
    "        filtered_tweet = []    \n",
    "        for w in word_tokens:\n",
    "        #check tokens against stop words , emoticons and punctuations\n",
    "            if w not in stop_words and w not in emoticons and w not in string.punctuation:\n",
    "                filtered_tweet.append(w)\n",
    "\n",
    "        return ' '.join(filtered_tweet) \n",
    "    \n",
    "\n",
    "    def get_user_data(self,target):\n",
    "    \n",
    "        #test if screen name exists \n",
    "        try:\n",
    "            #print(\"Getting data for \" + target)\n",
    "            item = self.api.get_user(target)  \n",
    "            \n",
    "            #derived params\n",
    "            tweets = item.statuses_count\n",
    "            account_created_date = item.created_at\n",
    "            delta = datetime.utcnow() - account_created_date\n",
    "            account_age_days = delta.days\n",
    "            average_tweet_per_day = float(tweets)/float(account_age_days)\n",
    "            \n",
    "            item = item._json\n",
    "            item['account_age_days'] = account_age_days\n",
    "            item['average_tweet_per_day'] = average_tweet_per_day\n",
    "            \n",
    "        except tweepy.TweepError as e:\n",
    "            print()\n",
    "            print(f\"ERROR CODE: {e.args[0][0]['code']}: {e.args[0][0]['message']}\")\n",
    "            print(f'issue with reading screen_name = {target}')\n",
    "            item = None\n",
    "            \n",
    "        return item\n",
    "            \n",
    "            \n",
    "    def get_screen_names(self,users, **kwargs):\n",
    "        \n",
    "        csv = kwargs.get('csv',False)\n",
    "        col = kwargs.get('col','twitter_handle')\n",
    "        nusers = kwargs.get('nusers',0)\n",
    "        \n",
    "        if csv:\n",
    "            try:\n",
    "                print(users)\n",
    "                df = pd.read_csv(users)\n",
    "                print(df.head())\n",
    "                users = df[col].to_list()\n",
    "            except:\n",
    "                print(f'failing to get users name from column={col} of file {users}' )\n",
    "                raise\n",
    "        else:\n",
    "            if not isinstance(users, (list, tuple, set)):\n",
    "                print('Provided users input is not list or tuple. The type(users)',type(users))\n",
    "            \n",
    "        #allow to get only few numbers\n",
    "        users_to_get = users\n",
    "        if nusers>0 and nusers<=len(users):\n",
    "            users_to_get = users[0:nusers]\n",
    "            \n",
    "        return users_to_get\n",
    "    \n",
    "    def get_users_info(self,users, \n",
    "                      csv=True, \n",
    "                      col='Name',                        \n",
    "                      nusers=0, \n",
    "                      fname=None,\n",
    "                      **kwargs):\n",
    "        '''\n",
    "        This function returns User Objects\n",
    "        The User object contains Twitter User account metadata \n",
    "        that describes the Twitter User referenced. Users can author \n",
    "        Tweets, Retweet, quote other Users Tweets, \n",
    "        reply to Tweets, follow Users, be @mentioned in \n",
    "        Tweets and can be grouped into lists.\n",
    "        \n",
    "        REF\n",
    "        https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/user-object\n",
    "        '''\n",
    "        users_to_get = self.get_screen_names(users,\n",
    "                                      csv=csv,\n",
    "                                      col=col, \n",
    "                                      nusers=nusers,\n",
    "                                      fname=None,\n",
    "                                      **kwargs)\n",
    "                        \n",
    "            \n",
    "        info_list = []\n",
    "        for target in users_to_get:\n",
    "            \n",
    "            item = self.get_user_data(target)\n",
    "            if item is None:\n",
    "                continue\n",
    "\n",
    "            if kwargs.get('verbose',self.verbose)>2:\n",
    "                print(\"name: \" + item['name'])\n",
    "                print(\"screen_name: \" + item['screen_name'])\n",
    "                print(\"description: \" + item['description'])\n",
    "                print(\"statuses_count: \" + str(item['statuses_count']))\n",
    "                print(\"friends_count: \" + str(item['friends_count']))\n",
    "                print(\"followers_count: \" + str(item['followers_count']))\n",
    "                print(\"account created date: \",item['created_at'])\n",
    "                print(\"Account age (in days): \" + str(item['account_age_days']))\n",
    "                if item['average_tweet_per_day'] > 0:\n",
    "                      print(\"Average tweets per day: \" + \"%.2f\"%(item['average_tweet_per_day'])) \n",
    "                                    \n",
    "\n",
    "            info_list.append(item)\n",
    "            \n",
    "        df = pd.DataFrame(info_list)\n",
    "        \n",
    "        #save\n",
    "        if not fname is None:\n",
    "            df.to_csv(fname)\n",
    "            \n",
    "        return df\n",
    "    \n",
    "    def print_most_common(self,t):\n",
    "        mentions = t['mentions']\n",
    "        hashtags = t['hashtags']\n",
    "        tweet_count = t['tweet_count']\n",
    "        \n",
    "        \n",
    "        print()\n",
    "        print(\"Most mentioned Twitter users:\")\n",
    "        for item, count in Counter(mentions).most_common(10):\n",
    "          print(item + \"\\t\" + str(count))\n",
    "\n",
    "        print()\n",
    "        print(\"Most used hashtags:\")\n",
    "        for item, count in Counter(hashtags).most_common(10):\n",
    "          print(item + \"\\t\" + str(count))\n",
    "\n",
    "        print()\n",
    "        print(\"All done. Processed \" + str(tweet_count) + \" tweets.\")\n",
    "\n",
    "    \n",
    "    def status_to_entities(self,status):\n",
    "        hashtags = []\n",
    "        mentions = []\n",
    "        \n",
    "        #get some attributes in separate list\n",
    "        if hasattr(status, \"entities\"):\n",
    "            entities = status.entities\n",
    "            #\n",
    "            if \"hashtags\" in entities:\n",
    "                for ent in entities[\"hashtags\"]:\n",
    "                    if ent is not None:\n",
    "                        if \"text\" in ent:\n",
    "                            hashtag = ent[\"text\"]\n",
    "                            if hashtag is not None:\n",
    "                                hashtags.append(hashtag)\n",
    "\n",
    "\n",
    "            if \"user_mentions\" in entities:\n",
    "                for ent in entities[\"user_mentions\"]:\n",
    "                    if ent is not None:\n",
    "                        if \"screen_name\" in ent:\n",
    "                            name = ent[\"screen_name\"]\n",
    "                            if name is not None:\n",
    "                                mentions.append(name) \n",
    "                            \n",
    "        return mentions, hashtags\n",
    "    \n",
    "    def entities_dict(self,status):\n",
    "        mentions, hashtags = self.status_to_entities(status)\n",
    "        \n",
    "        #\n",
    "        entdict = {'nmention':len(mentions), 'nhashtag':len(hashtags),\n",
    "                   'mentions':'|'.join(mentions),'hashtags':'|'.join(hashtags)}\n",
    "        \n",
    "        #\n",
    "#         for item, count in Counter(mentions).most_common(10):\n",
    "#             entdict['%s_mentions'%item] = count\n",
    "            \n",
    "#         for item, count in Counter(hashtags).most_common(10):\n",
    "#             entdict['%s_count'%item] = count    \n",
    "            \n",
    "        return entdict\n",
    "    \n",
    "    \n",
    "    def get_all_tweets(self,screen_name, \n",
    "                       fname=None,\n",
    "                       ndays=10000, \n",
    "                       nitem=10000):\n",
    "        #Twitter only allows access to a users most recent 3240 tweets with this method\n",
    "        '''\n",
    "        Entities provide metadata and additional contextual information \n",
    "        about content posted on Twitter. The entities section provides \n",
    "        arrays of common things included in Tweets: \n",
    "        hashtags, user mentions, links, stock tickers (symbols),\n",
    "        Twitter polls, and attached media. \n",
    "        \n",
    "        More Ref:\n",
    "        https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/entities-object\n",
    "        '''\n",
    "        #\n",
    "        name = screen_name.replace('@','')\n",
    "        \n",
    "        #\n",
    "        item = self.get_user_data(screen_name)\n",
    "        if item is None:\n",
    "            return []\n",
    "\n",
    "                \n",
    "        if fname is None:\n",
    "            name = screen_name.replace('@','')\n",
    "            fname = f'users_tweets/{name}_tweets.json'\n",
    "            \n",
    "        fhandle = open(fname, 'w')\n",
    "        \n",
    "        #initialize a list to hold all the tweepy Tweets\n",
    "        alltweets = []  #tweet object sotre\n",
    "        allentities = []  #entities object store\n",
    "\n",
    "        \n",
    "        #filter tweets by date\n",
    "        if ndays>0:\n",
    "            end_date = datetime.utcnow() - timedelta(days=ndays)\n",
    "        else:\n",
    "            end_date = None\n",
    "        \n",
    "        \n",
    "        #iterate on status\n",
    "        tweet_count = 0\n",
    "        pbar = tqdm(total=3200) \n",
    "        for tweet_obj in tweepy.Cursor(self.api.user_timeline, id=screen_name).items(nitem):\n",
    "            tweet_count += 1\n",
    "            \n",
    "            #get dict\n",
    "            status = tweet_obj._json\n",
    "            \n",
    "            alltweets.append(status)   \n",
    "            \n",
    "            #write to json \n",
    "            json.dump(status,fhandle)\n",
    "            fhandle.write('\\n')\n",
    "            \n",
    "            #progress bar\n",
    "            pbar.set_description(f\"{name}: {tweet_count} tweets downloaded so far\")\n",
    "            #pbar.update(10)\n",
    "                \n",
    "            #apply date filter\n",
    "            if not end_date is None:\n",
    "                if status.created_at < end_date:\n",
    "                    break  \n",
    "                    \n",
    "            \n",
    "            \n",
    "        \n",
    "        #write to json        \n",
    "        #with open(fname, 'w') as f:\n",
    "        #    json.dump(alltweets,f)\n",
    "        fhandle.close()\n",
    "        pbar.close()\n",
    "        \n",
    "        return alltweets\n",
    "\n",
    "            \n",
    "    def get_users_tweet(self, users, \n",
    "                        csv=True,\n",
    "                        col='twitter_handle',                        \n",
    "                        nusers=0, \n",
    "                        overwrite=False,\n",
    "                        call_api=False,\n",
    "                        **kwargs):\n",
    "        #\n",
    "        #\n",
    "        users_to_get = self.get_screen_names(users,\n",
    "                                      csv=csv,\n",
    "                                      col=col, \n",
    "                                      nusers=nusers)\n",
    "        \n",
    "        alluserstweets = []\n",
    "        #\n",
    "        for screen_name in users_to_get:\n",
    "            name = screen_name.replace('@','')\n",
    "            fname = f'users_tweets/{name}_tweets.json'\n",
    "            #print(fname, 'file exists: ',os.path.exists(fname))\n",
    "            \n",
    "            #\n",
    "            if os.path.exists(fname) and not overwrite:\n",
    "                #\n",
    "                alltweets = read_tweet_json(fname) #json.load(open(fname,'r'))\n",
    "                if len(alltweets)==0:\n",
    "                    print(f\"No lines found in file {fname}\")\n",
    "                    continue\n",
    "                \n",
    "            else:\n",
    "                if call_api:\n",
    "                    try:\n",
    "                        alltweets = self.get_all_tweets(screen_name,**kwargs)\n",
    "                    except tweepy.TweepError as e:\n",
    "                        print(f\"Skipping {screen_name} .. error: {e.args[0][0]['message']}\") \n",
    "                        continue\n",
    "                else:\n",
    "                    print(f\"Skipping {screen_name} .. no local data found and call_api=False\")\n",
    "                    continue\n",
    "\n",
    "                \n",
    "            if len(alltweets)==0:\n",
    "                continue\n",
    "            else:\n",
    "                #transform the tweepy tweets into a 2D array that will populate DataFrame \n",
    "                outtweets = [{'id':tweet[\"id_str\"], \n",
    "                              'created_at':tweet[\"created_at\"], \n",
    "                              'retweet_count':tweet[\"retweet_count\"],\n",
    "                              'favorite_count':tweet[\"favorite_count\"],\n",
    "                              'is_retweet': 'retweeted_status' in tweet.keys(),\n",
    "                              'is_quoted': tweet[\"is_quote_status\"],\n",
    "                              'reply_to_status': tweet[\"in_reply_to_status_id_str\"],\n",
    "                              'reply_to_user':tweet[\"in_reply_to_user_id_str\"],\n",
    "                              'reply_to_username':tweet[\"in_reply_to_screen_name\"],\n",
    "                              'coord':tweet[\"coordinates\"],\n",
    "                              'source':tweet[\"source\"],\n",
    "                              'truncated': tweet[\"truncated\"],                              \n",
    "                              'text':tweet[\"text\"], \n",
    "                              **self.entities_dict(tweet)} for tweet in alltweets]\n",
    "\n",
    "                alluserstweets.append(pd.DataFrame(outtweets))\n",
    "            \n",
    "        return alluserstweets\n",
    "            \n",
    "            \n",
    "    def status_sentiment_info(self,status, df=None):\n",
    "\n",
    "        #if this tweet is a retweet update retweet count\n",
    "        if not df is None:\n",
    "            if status['created_at'] in df['created_at'].values:\n",
    "                i = df.loc[df['created_at'] == status['created_at']].index[0]\n",
    "                #\n",
    "                cond1 = status['favorite_count'] != df.at[i, 'favorite_count']\n",
    "                cond2 = status['retweet_count'] != df.at[i, 'retweet_count']\n",
    "                if cond1 or cond2:\n",
    "                    df.at[i, 'favorite_count'] = status['favorite_count']\n",
    "                    df.at[i, 'retweet_count'] = status['retweet_count']\n",
    "                    \n",
    "            return []\n",
    "\n",
    "        #calculate sentiment\n",
    "        filtered_tweet = self.clean_tweets(status['full_text'])\n",
    "        blob = TextBlob(filtered_tweet)\n",
    "        Sentiment = blob.sentiment     \n",
    "        polarity = Sentiment.polarity\n",
    "        subjectivity = Sentiment.subjectivity\n",
    "\n",
    "        \n",
    "        new_entry = [status['id'], status['created_at'],\n",
    "                      status['source'], status['full_text'], filtered_tweet, \n",
    "                      Sentiment,polarity,subjectivity, status['lang'],\n",
    "                      status['favorite_count'], status['retweet_count']]\n",
    "\n",
    "        new_entry.append(status['user']['screen_name'])\n",
    "\n",
    "        try:\n",
    "            is_sensitive = status['possibly_sensitive']\n",
    "        except KeyError:\n",
    "            is_sensitive = None\n",
    "\n",
    "        new_entry.append(is_sensitive)\n",
    "\n",
    "        hashtags = \", \".join([hashtag_item['text'] for hashtag_item in \\\n",
    "                              status['entities']['hashtags']])\n",
    "        new_entry.append(hashtags) #append the hashtags\n",
    "\n",
    "        #\n",
    "        mentions = \", \".join([mention['screen_name'] for mention in \\\n",
    "                              status['entities']['user_mentions']])\n",
    "        new_entry.append(mentions) #append the user mentions\n",
    "\n",
    "        try:\n",
    "            xyz = status['place']['bounding_box']['coordinates']\n",
    "            coordinates = [coord for loc in xyz for coord in loc]\n",
    "        except TypeError:\n",
    "            coordinates = None\n",
    "        #\n",
    "        new_entry.append(coordinates)\n",
    "\n",
    "        try:\n",
    "            location = status['user']['location']\n",
    "        except TypeError:\n",
    "            location = ''\n",
    "        #\n",
    "        new_entry.append(location)  \n",
    "        \n",
    "        return new_entry\n",
    "            \n",
    "    def get_tweets(self, keyword, fname, csvfile=None, \n",
    "                   append=False, npages=-1,ncount=100,\n",
    "                   overwrite=False):\n",
    "        \n",
    "        \n",
    "        df = pd.DataFrame(columns=self.cols)\n",
    "        \n",
    "        #read csv and return if append is False\n",
    "        if not csvfile is None:\n",
    "            #If the file exists, then read the existing data from the CSV file.\n",
    "            if os.path.exists(csvfile):\n",
    "                df = pd.read_csv(csvfile, header=0)\n",
    "            \n",
    "        #progress bar initiate\n",
    "        icount = 0\n",
    "        ipage = 0\n",
    "        pbar = tqdm(total=ncount*int(max([40,ncount])) )\n",
    "        print(f'search & download with keyword: {keyword}')\n",
    "        \n",
    "        alltweets = []\n",
    "        fhandle = None\n",
    "        \n",
    "        #check if json file exists\n",
    "        if os.path.exists(fname) and not overwrite:\n",
    "            if append:\n",
    "                fhandle = open(fname, 'a')\n",
    "                #add new raw tweets to json and build on df\n",
    "            else:    \n",
    "                #\n",
    "                df = pd.DataFrame(columns=self.cols) #overwrite saved df\n",
    "                all_raw_weets = read_tweet_json(fname)    \n",
    "                alltweets = [self.status_sentiment_info(status) for x in all_raw_weets]\n",
    "            \n",
    "\n",
    "        else:      \n",
    "            #call api, get data, save tweets to json\n",
    "            \n",
    "            #if \n",
    "            if fhandle is None:             \n",
    "                fhandle = open(fname, 'w')                \n",
    "            \n",
    "            \n",
    "            #page attribute in tweepy.cursor and iteration\n",
    "            for page in tweepy.Cursor(self.api.search, q=keyword,count=ncount, \n",
    "                                      include_rts=False,tweet_mode='extended').pages(npages):\n",
    "\n",
    "                # the you receive from the Twitter API is in a JSON format and has quite an \n",
    "                # amount of information attached\n",
    "                for status in page:\n",
    "\n",
    "\n",
    "                    status = status._json\n",
    "\n",
    "                    #write to json \n",
    "                    json.dump(status,fhandle)\n",
    "                    fhandle.write('\\n')\n",
    "\n",
    "                    #filter by language\n",
    "                    #if status['lang'] != 'en':\n",
    "                    #    continue\n",
    "\n",
    "                    new_entry = self.status_sentiment_info(status,df=df)\n",
    "                    if len(new_entry)>0:\n",
    "                        alltweets.append(new_entry)\n",
    "\n",
    "                    #info                \n",
    "                    #pbar.set_description(f\"tweets downloaded so far: {icount}\")\n",
    "                    #pbar.update(1)\n",
    "                    icount += 1\n",
    "\n",
    "                ipage += 1\n",
    "                print(f'page={ipage}, #tweet={icount}')\n",
    "            \n",
    "\n",
    "        #close open states\n",
    "        print(f'keyword: {keyword}')\n",
    "        print(f\"total #tweets downloaded: {icount}\")\n",
    "        pbar.close()\n",
    "        \n",
    "        #close file if open\n",
    "        if not fhandle is None:\n",
    "            fhandle.close()\n",
    "        \n",
    "        \n",
    "        #now append a row to the dataframe\n",
    "        df_new = pd.DataFrame(alltweets, columns=self.cols)\n",
    "        df = pd.concat([df_new, df]).drop_duplicates()\n",
    "        \n",
    "    \n",
    "        #\n",
    "        df['timestamp'] = df.created_at.map(pd.Timestamp)\n",
    "        df = df.sort_values('timestamp').set_index('timestamp')\n",
    "        df = df.drop('id',axis=1)\n",
    "        \n",
    "        if not csvfile is None:\n",
    "            #save it to file\n",
    "            df.to_csv(csvfile, columns=self.cols, index=True, encoding=\"utf-8\")\n",
    "            \n",
    "\n",
    "        return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-10x]",
   "language": "python",
   "name": "conda-env-.conda-10x-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
