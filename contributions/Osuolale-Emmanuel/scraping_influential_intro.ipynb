{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "scraping_influential_intro.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6HuiCzraVZF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%%writefile ../pyscrap_url.py\n",
        "\n",
        "def simple_get(url):\n",
        "    \"\"\"\n",
        "    Attempts to get the content at `url` by making an HTTP GET request.\n",
        "    If the content-type of response is some kind of HTML/XML, return the\n",
        "    text content, otherwise return None.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with closing(get(url, stream=True)) as resp:\n",
        "            if is_good_response(resp):\n",
        "                return resp.content  #.encode(BeautifulSoup.original_encoding)\n",
        "            else:\n",
        "                return None\n",
        "\n",
        "    except RequestException as e:\n",
        "        log_error('Error during requests to {0} : {1}'.format(url, str(e)))\n",
        "        return None\n",
        "\n",
        "\n",
        "def is_good_response(resp):\n",
        "    \"\"\"\n",
        "    Returns True if the response seems to be HTML, False otherwise.\n",
        "    \"\"\"\n",
        "    content_type = resp.headers['Content-Type'].lower()\n",
        "    return (resp.status_code == 200 \n",
        "            and content_type is not None \n",
        "            and content_type.find('html') > -1)\n",
        "\n",
        "\n",
        "def log_error(e):\n",
        "    \"\"\"\n",
        "    It is always a good idea to log errors. \n",
        "    This function just prints them, but you can\n",
        "    make it do anything.\n",
        "    \"\"\"\n",
        "    print(e)\n",
        "    \n",
        "def get_elements(url, tag='',search={}, fname=None):\n",
        "    \"\"\"\n",
        "    Downloads a page specified by the url parameter\n",
        "    and returns a list of strings, one per tag element\n",
        "    \"\"\"\n",
        "    \n",
        "    if isinstance(url,str):\n",
        "        response = simple_get(url)\n",
        "    else:\n",
        "        #if already it is a loaded html page\n",
        "        response = url\n",
        "\n",
        "    if response is not None:\n",
        "        html = BeautifulSoup(response, 'html.parser')\n",
        "        \n",
        "        res = []\n",
        "        if tag:    \n",
        "            for li in html.select(tag):\n",
        "                for name in li.text.split('\\n'):\n",
        "                    if len(name) > 0:\n",
        "                        res.append(name.strip())\n",
        "                       \n",
        "                \n",
        "        if search:\n",
        "            soup = html            \n",
        "            \n",
        "            \n",
        "            r = ''\n",
        "            if 'find' in search.keys():\n",
        "                print('findaing',search['find'])\n",
        "                soup = soup.find(**search['find'])\n",
        "                r = soup\n",
        "\n",
        "                \n",
        "            if 'find_all' in search.keys():\n",
        "                print('findaing all of',search['find_all'])\n",
        "                r = soup.find_all(**search['find_all'])\n",
        "   \n",
        "            if r:\n",
        "                for x in list(r):\n",
        "                    if len(x) > 0:\n",
        "                        res.extend(x)\n",
        "            \n",
        "        return res\n",
        "\n",
        "    # Raise an exception if we failed to get any data from the url\n",
        "    raise Exception('Error retrieving contents at {}'.format(url))    \n",
        "    \n",
        "    \n",
        "if get_ipython().__class__.__name__ == '__main__':\n",
        "    fire(get_tag_elements)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYo_A65EaXzm",
        "colab_type": "text"
      },
      "source": [
        "### Here i would be sharing Ideas on how to scrape the influencer data for the website we were given"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPB1CpTy1Fif",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#I started with using our get element to scrape the data from our website \n",
        "res = get_elements('https://africafreak.com/100-most-influential-twitter-users-in-africa',tag='h2')\n",
        "res[:3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8X7dYy0D1uGq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Then i proceeded to get the first character which is the numbers \n",
        "index = []\n",
        "for char in numbers:\n",
        "  index.append(char[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UA-HhrOHkubl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Here is how i split the second data \n",
        "names =[]\n",
        "for name in numbers:\n",
        "  names.append(name[-1].split(\"(\"))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYCceI7b2rt0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Here i worked with the fullname of the influencers\n",
        "fullname = []\n",
        "for name in names:\n",
        "  fullname.append(name[0].strip())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvVxcdd6sNK0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Then their Handles \n",
        "handle =[]\n",
        "for name in names:\n",
        "  handle.append(name[-1][:-2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkaVO19G3PlX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "handle[:3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBAjs6zzbQkZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#You can continue from where i stopped here, thank you for reading this "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}